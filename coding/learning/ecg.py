# -*- coding: utf-8 -*-
"""ECG.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qyhjhjcAa9q87zXtEfUT6VcMDvs_1KAB

## Imports
"""

import glob
import PIL.Image as Image
import os
from torchvision.datasets import ImageFolder
import matplotlib.pyplot as plt
import torch
import numpy as np
from torch import nn
import torch.nn.functional as F
import pytorch_lightning as pl
from pytorch_lightning.callbacks import Callback, ModelCheckpoint
from pytorch_lightning.callbacks.early_stopping import EarlyStopping
from pytorch_lightning.metrics.functional import accuracy
from pytorch_lightning.loggers import TensorBoardLogger
from torchvision import transforms
from torch.utils.data import DataLoader, random_split, Dataset
from torchvision.models import resnet18
import wfdb
import ast
import pandas as pd
import pickle

# setting device on GPU if available, else CPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print('Using device:', device)
if device == torch.device('cuda'):
  print(torch.cuda.get_device_name(0))

# Reproducibility
seed = 7
torch.manual_seed(seed)
np.random.seed(seed)

"""## Data"""


class OriginalData(Dataset):
    def __init__(self, path, transform=None):

        # # load and convert annotation data
        # Y = pd.read_csv(path + 'ptbxl_database.csv', index_col='ecg_id')
        # Y.scp_codes = Y.scp_codes.apply(lambda x: ast.literal_eval(x))
        #
        # # Load raw signal data
        # sampling_rate = 100
        # X = self.load_raw_data(Y, sampling_rate, path)
        #
        # # Load scp_statements.csv for diagnostic aggregation
        self.agg_df = pd.read_csv(path + 'scp_statements.csv', index_col=0)
        self.agg_df = self.agg_df[self.agg_df.diagnostic == 1]


        # with open(path + 'Y_metadata.pickle', 'wb') as handle:
        #     pickle.dump(Y, handle, protocol=pickle.HIGHEST_PROTOCOL)

        with open(path + 'signals.pickle', 'rb') as handle:
            X = pickle.load(handle)
        with open(path + 'Y_metadata.pickle', 'rb') as handle:
            Y = pickle.load(handle)

        classes_dict = {"CD": 0, "HYP": 1, "MI": 2, "NORM": 3, "STTC": 4}
        # Apply diagnostic superclass
        Y['diagnostic_superclass'] = Y.scp_codes.apply(self.aggregate_diagnostic)
        Ya = Y['diagnostic_superclass'].to_numpy()
        Yb = np.array([classes_dict[y[0]] if len(y) == 1 else None for y in Ya])

        max_inds_per_class = 2500
        dataset_inds = []
        for curr_class in classes_dict.values():
            class_inds = np.where(Yb == curr_class)[0]
            np.random.shuffle(class_inds)
            dataset_inds.extend(class_inds[:max_inds_per_class])
        dataset_inds = np.array(dataset_inds)

        self.data = X[dataset_inds, 0, :]  # take only the 0 measurement for now
        self.targets = torch.LongTensor(Yb[dataset_inds].astype(np.int8))

        # proper_label_inds = np.where(Yb != None)[0]
        # self.data = X[proper_label_inds, 0, :]  # take only the 0 measurement for now
        # self.targets = torch.LongTensor(Yb[proper_label_inds].astype(np.int8))

        # Split data into train and test
        # test_fold = 10
        # Train
        # X_train = X[np.where(Y.strat_fold != test_fold)]
        # y_train = Y[(Y.strat_fold != test_fold)].diagnostic_superclass
        # Test
        # X_test = X[np.where(Y.strat_fold == test_fold)]
        # y_test = Y[Y.strat_fold == test_fold].diagnostic_superclass
        # y_single_label = []

        # self.data = X_train
        # self.targets = torch.LongTensor(y_train)
        self.transform = transform

    def __getitem__(self, index):
        x = self.data[index]
        y = self.targets[index]

        x = torch.Tensor(x)
        x = (x - x.mean())/torch.std(x)
        # if self.transform:
        #     x = self.transform(x)

        return x, y

    def __len__(self):
        return len(self.data)

    def load_raw_data(self, df, sampling_rate, path):
        if sampling_rate == 100:
            data = [wfdb.rdsamp(path + f) for f in df.filename_lr]
        else:
            data = [wfdb.rdsamp(path + f) for f in df.filename_hr]
        data = np.array([signal for signal, meta in data])
        return data

    def aggregate_diagnostic(self, y_dic):
        tmp = []
        for key in y_dic.keys():
            if key in self.agg_df.index:
                tmp.append(self.agg_df.loc[key].diagnostic_class)
        return list(set(tmp))


class WaveletData(Dataset):
    def __init__(self, path, transform=None):
        classes_names = os.listdir(path)
        self.data = []
        self.targets = []
        for ii, curr_class in enumerate(classes_names):
            file_name = glob.glob(os.path.join(path, curr_class, '*'))
            np.random.shuffle(file_name)
            file_name = file_name[:]
            self.targets.extend([ii]*len(file_name))
            self.data.extend(file_name)

        self.targets = torch.LongTensor(self.targets)
        self.transform = transform

    def __getitem__(self, index):
        x = loader(self.data[index])
        y = self.targets[index]

        x = torch.Tensor(x)
        # x = (x - x.mean())/torch.std(x)
        # if self.transform:
        #     x = self.transform(x)

        return x, y

    def __len__(self):
        return len(self.data)


class STFTDataModule(pl.LightningDataModule):
    def __init__(self, batch_size, image_folder_path, transform=None):
        super().__init__()

        self.batch_size = batch_size
        self.image_folder_path = image_folder_path

        self.transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Grayscale(num_output_channels=1),
            transforms.Normalize((0.5), (0.5))
        ])

    def prepare_data(self):
        # download
        # PtbData(self.data_map_url, self.gender, self.under_50, self.state, download=True, transform=self.transform)
        pass

    def setup(self,stage=None):
        # Assign train/val datasets for use in dataloaders
        dataset = ImageFolder(self.image_folder_path, self.transform)
        if stage == 'train' or stage is None:
            self.train, self.val = random_split(dataset,
                                                [int(len(dataset)*0.8),
                                                 len(dataset) - int(len(dataset)*0.8)])
            # ptb_full = PtbData(self.data_map_url, self.gender, self.under_50, 'train', transform=self.transform)
            # self.train, self.val = random_split(ptb_full, [round(len(ptb_full)*0.85), len(ptb_full) - round(len(ptb_full)*0.85)])
            print('len all data: ', len(dataset))
            print('len train: ', len(self.train))
            print('len val: ', len(self.val))

        # Assign test dataset for use in dataloader(s)
        if stage == 'test' or stage is None:
            self.test = dataset if stage == 'test' else self.val
            # self.test = PtbData(self.data_map_url, self.gender, self.under_50, 'test', transform=self.transform)
            print('len test: ', len(self.test))

    def train_dataloader(self):
        return DataLoader(self.train, batch_size=self.batch_size, shuffle=True, drop_last=True, num_workers=4)

    def val_dataloader(self):
        return DataLoader(self.val, batch_size=self.batch_size, num_workers=4)

    def test_dataloader(self):
        return DataLoader(self.test, batch_size=self.batch_size, num_workers=4)


class OneDimDataModule(pl.LightningDataModule):
    def __init__(self, batch_size, folder_path, transform=None):
        super().__init__()

        self.batch_size = batch_size
        self.folder_path = folder_path

        self.transform = transforms.Compose([
            transforms.ToTensor(),
            #transforms.Grayscale(num_output_channels=12),
            # transforms.Normalize((0.5), (0.5))
        ])

    def prepare_data(self):
        pass

    def setup(self,stage=None):
        # Assign train/val datasets for use in dataloaders
        dataset = self.get_data(mode)
        if stage == 'train' or stage is None:
            self.train, self.val = random_split(dataset,
                                                [int(len(dataset)*0.8),
                                                 len(dataset) - int(len(dataset)*0.8)])
            print('len all data: ', len(dataset))
            print('len train: ', len(self.train))
            print('len val: ', len(self.val))

        # Assign test dataset for use in dataloader(s)
        if stage == 'test' or stage is None:
            self.test = dataset if stage == 'test' else self.val
            print('len test: ', len(self.test))

    def train_dataloader(self):
        return DataLoader(self.train, batch_size=self.batch_size, shuffle=True, drop_last=True, num_workers=4)

    def val_dataloader(self):
        return DataLoader(self.val, batch_size=self.batch_size, num_workers=4)

    def test_dataloader(self):
        return DataLoader(self.test, batch_size=self.batch_size, num_workers=4)

    def get_data(self, mode):
        if mode in ('Hwavelet', 'Dwavelet', 'Mwavelet' ):
            return WaveletData(self.folder_path, self.transform)
        else:
            return OriginalData(self.folder_path, self.transform)


class PaperNet(pl.LightningModule):
    """## Model"""
    def __init__(self, input_shape, num_classes, loss_weights_train,loss_weights_val, device, learning_rate, weight_decay, batch_size, drop_prob=0, num_features_fc=13):
        super().__init__()

        # log hyper-parameters
        self.learning_rate = learning_rate
        self.loss_weights_train = loss_weights_train
        self.loss_weights_val = loss_weights_val
        self.weight_decay = weight_decay
        self.batch_size = batch_size
        self.drop_prob = drop_prob
        self.num_features_fc = num_features_fc
        self.input_shape = input_shape
        # self.device = device
        self.save_hyperparameters()

        self.pre_process = self.get_pre_process().to(device)
        self.features = nn.Sequential(
            nn.Conv2d(1, 8, 4),
            nn.BatchNorm2d(8),
            nn.ReLU(),
            # nn.MaxPool2d(2),
            # nn.Conv2d(8, 13, 3),
            # nn.BatchNorm2d(13),
            # nn.ReLU(),
            # nn.MaxPool2d(2),
            # nn.Conv2d(13, 13, 3),
            # nn.BatchNorm2d(13),
            nn.ReLU(),
        ).to(device)

        self.features_num = self._get_conv_output(input_shape)

        self.classifier = nn.Sequential(
            nn.Linear(self.features_num, self.num_features_fc),
            nn.BatchNorm1d(self.num_features_fc),
            nn.Dropout2d(p=drop_prob),
            nn.Linear(self.num_features_fc, num_classes),
            nn.Softmax(-1)
        ).to(device)


    def __str__(self):
        return "PaperNet"

    # returns the size of the output tensor going into Linear layer from the conv block.
    def _get_conv_output(self, shape):
        batch_size = 1
        input = torch.autograd.Variable(torch.rand(batch_size, *shape)).cuda()

        output_feat = self.features(self.pre_process(input))
        n_size = output_feat.data.view(batch_size, -1).size(1)
        return n_size

    def get_pre_process(self):
        return nn.Identity()

    def forward(self, x):
        x = self.pre_process(x)
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

    def training_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = F.nll_loss(logits, y, weight=self.loss_weights_train)

        # training metrics
        # preds = torch.argmax(logits, dim=1)
        # acc = accuracy(preds, y)
        # self.log('train_loss11', loss, on_step=True, on_epoch=True, logger=True)
        # self.log('train_acc11', acc, on_step=True, on_epoch=True, logger=True)

        correct = (torch.argmax(logits, -1) == y).sum()
        total = torch.numel(y)

        batch_dict = {
            "loss": loss,
            'correct': correct,
            "total": total
        }

        return batch_dict

    def training_epoch_end(self, outputs):
        #  the function is called after every epoch is completed

        # calculating average loss
        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()

        # calculating correct and total predictions
        correct = sum([x["correct"] for x in outputs])
        total = sum([x["total"] for x in outputs])
        acc = correct / total

        self.logger.experiment.add_scalar('train_loss Epoch', avg_loss, self.trainer.current_epoch)
        self.logger.experiment.add_scalar('train_acc Epoch', acc, self.trainer.current_epoch)

    def validation_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        
        loss = F.nll_loss(logits, y, weight=self.loss_weights_val)

        correct = (torch.argmax(logits, -1) == y).sum()
        total = torch.numel(y)

        batch_dict = {
            "loss": loss,
            'correct': correct,
            "total": total
        }

        return batch_dict

    def validation_epoch_end(self, outputs):
        #  the function is called after every epoch is completed

        # calculating average loss
        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()

        # calculating correct and total predictions
        correct = sum([x["correct"] for x in outputs])
        total = sum([x["total"] for x in outputs])
        acc = correct / total

        self.logger.experiment.add_scalar('val_loss Epoch', avg_loss, self.trainer.current_epoch)
        self.logger.experiment.add_scalar('val_acc Epoch', acc, self.trainer.current_epoch)

    def test_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = F.nll_loss(logits, y)

        # validation metrics
        preds = torch.argmax(logits, dim=1)
        acc = accuracy(preds, y)
        self.log('test_loss', loss, prog_bar=True)
        self.log('test_acc', acc, prog_bar=True)

        correct = (torch.argmax(logits, -1) == y).sum()
        total = torch.numel(y)

        batch_dict = {
            "loss": loss,
            'correct': correct,
            "total": total
        }

        return batch_dict

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)
        return optimizer


class OneDimNet(PaperNet):
    # go to 3.2.3
    #https://ataspinar.com/2018/12/21/a-guide-for-using-the-wavelet-transform-in-machine-learning/https://ataspinar.com/2018/12/21/a-guide-for-using-the-wavelet-transform-in-machine-learning/
    def __init__(self, input_shape, num_classes, loss_weights_train, loss_weights_val, device, learning_rate, weight_decay, batch_size, drop_prob, feature_num):
        super().__init__(input_shape, num_classes, loss_weights_train, loss_weights_val, device, learning_rate, weight_decay, batch_size, drop_prob, feature_num)
        self.features = nn.Sequential(
            nn.Conv2d(input_shape[0],32,5),
            nn.ReLU(),
            nn.MaxPool2d(2, stride=2),
            nn.Conv2d(32,64,5),
            nn.ReLU(),
            nn.MaxPool2d(2, stride=2),
            nn.Flatten()
        )

        self.classifier = nn.Sequential(
            nn.Linear(1000,num_classes),
            nn.ReLU(),
            nn.Softmax(-1)
        )

    def __str__(self):
        return "OneDimNet"

    def forward(self, x):
        x = x.squeeze()
        x = self.features(x)
        x = self.classifier(x)
        return x

    def _get_conv_output(self, shape):
        return 10  # irrelevant uint


class OneDimConvNet(PaperNet):
    def __init__(self, input_shape, num_classes, loss_weights_train, loss_weights_val, device, learning_rate, weight_decay, batch_size, drop_prob, feature_num):
        super().__init__(input_shape, num_classes, loss_weights_train, loss_weights_val, device, learning_rate, weight_decay, batch_size, drop_prob, feature_num)

    def __str__(self):
        return "OneDimConvNet"

    def forward(self, x):
        x = self.pre_process(x.unsqueeze(1)).unsqueeze(1)
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

    def _get_conv_output(self, shape):
        batch_size = 1
        input = torch.autograd.Variable(torch.rand(batch_size, *shape)).cuda()

        output_feat = self.features(self.pre_process(input).unsqueeze(0))
        n_size = output_feat.data.view(batch_size, -1).size(1)
        return n_size

    def get_pre_process(self):
        return nn.Conv1d(1, 32, 5, 2)


class ResNet(PaperNet):
    def __init__(self, input_shape, num_classes, loss_weights, device, learning_rate, weight_decay, batch_size, drop_prob, feature_num):
        super().__init__(input_shape, num_classes, loss_weights, device, learning_rate, weight_decay, batch_size, drop_prob, feature_num)

        self.resnet = resnet18(num_classes=num_classes)

    def forward(self, x):
        x = self.pre_process(x.unsqueeze(1)).unsqueeze(1)
        # x =
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

    def _get_conv_output(self, shape):
        batch_size = 1
        input = torch.autograd.Variable(torch.rand(batch_size, *shape)).cuda()

        output_feat = self.features(self.pre_process(input).unsqueeze(0))
        n_size = output_feat.data.view(batch_size, -1).size(1)
        return n_size

    def get_pre_process(self):
        return nn.Conv1d(1, 256, 3, 1)


class LSTM(OneDimNet):
    def __init__(self, input_shape, num_classes, loss_weights_train, loss_weights_val, device, learning_rate, weight_decay, batch_size, drop_prob, feature_num):
        super().__init__(input_shape, num_classes, loss_weights_train, loss_weights_val, device, learning_rate, weight_decay, batch_size, drop_prob, feature_num)


    def __str__(self):
        return "LSTM"

    def get_pre_process(self):
        return nn.GRU(input_size=self.input_shape[1], hidden_size=13, num_layers=6, batch_first=True, dropout=self.drop_prob)


class ImagePredictionLogger(Callback):
    def __init__(self, val_samples, classes_names, num_samples=20):
        super().__init__()
        self.num_samples = num_samples
        self.val_imgs, self.val_labels = val_samples
        self.classes_names = classes_names

    def on_validation_epoch_end(self, trainer, pl_module):
        # Bring the tensors to CPU
        val_imgs = self.val_imgs.to(device=pl_module.device)
        val_labels = self.classes_names[self.val_labels]
        # Get model prediction
        logits = pl_module(val_imgs)
        preds = self.classes_names[torch.argmax(logits, -1).cpu()]
        # Log the images

        fig = image_grid(val_imgs[:self.num_samples], val_labels[:self.num_samples], preds[:self.num_samples])
        trainer.logger.experiment.add_figure('Predictions', fig, global_step=trainer.current_epoch)
        # trainer.logger.experiment.log({
        #     "examples": [Image(x, caption=f"Pred:{pred}, Label:{y}")
        #                  for x, pred, y in zip(val_imgs[:self.num_samples],
        #                                        preds[:self.num_samples],
        #                                        val_labels[:self.num_samples])]
        # })


def image_grid(images, labels, preds):
    figure = plt.figure(figsize=(12, 8))

    num_imgs_to_plot = 16
    for ii, x, y, pred in zip(range(num_imgs_to_plot), images[:num_imgs_to_plot],
                              labels[:num_imgs_to_plot], preds[:num_imgs_to_plot]):
        plt.subplot(int(np.sqrt(num_imgs_to_plot)), int(np.sqrt(num_imgs_to_plot)), ii + 1)
        plt.title('label: ' + y + '. pred: ' + pred)
        plt.xticks([])
        plt.yticks([])
        if x.dim() == 3:
            plt.grid(False)
            plt.imshow(x[0].cpu().numpy(), cmap=plt.cm.coolwarm)
        else:
            plt.grid()
            plt.plot(x.cpu().numpy())

    return figure


def loader(path):
    s = " ".join(open(path, "r").read().split())
    wavelet = ast.literal_eval(s.replace('\n', '').replace(' ', ','))
    return wavelet


def get_data_module(mode, batch_size, data_path):
    if mode in ('Hwavelet', 'Dwavelet', 'Mwavelet', 'Original'):
        dm = OneDimDataModule(batch_size, data_path)
    elif mode == 'STFT':
        dm = STFTDataModule(batch_size, data_path)
    return dm


def get_model(mode, input_shape, num_classes, loss_weights_train, loss_weights_val, device, lr, weight_decay, batch_size, drop_prob, feature_num):
    if mode in ('Hwavelet', 'Dwavelet', 'Mwavelet','Original'):
        model = OneDimNet(input_shape, num_classes, loss_weights_train, loss_weights_val,device, lr, weight_decay, batch_size, drop_prob, feature_num)
        #model = OneDimConvNet(input_shape, num_classes, loss_weights_train, loss_weights_val, device, lr, weight_decay, batch_size, drop_prob, feature_num)
        #model = LSTM(input_shape, num_classes, loss_weights_train, loss_weights_val, device, lr, weight_decay, batch_size, drop_prob, feature_num)

    elif mode == 'STFT':
        model = PaperNet(input_shape, num_classes, loss_weights_train, loss_weights_val, device, lr, weight_decay, batch_size, drop_prob, feature_num)

    return model


mode = 'Mwavelet'  # 'Mwavelet' 'Hwavelet' 'STFT' 'Dwavelet' 'Original'
if mode == 'Hwavelet':
    data_path = 'HaarWavelet/full_with_single' #  '/inputs/TAU/SP/data/wavelets/HaarWavelet'  # '/inputs/TAU/SP/data/STFT' (all data)  # '/inputs/TAU/SP/data/wavelets/Hwavelet/only men multiple/ONLY_MEN_HAAR' (only male)
    input_shape = (1, 1000)
elif mode == 'Dwavelet':
    data_path = 'Daubechies6Wavelet/full_with_single'
    input_shape = (1, 1000)
elif mode == 'Mwavelet':
    data_path = 'MorlWavelet/full_with_single'
    input_shape = (12, 256, 256)
elif mode == 'STFT':
    data_path = '/inputs/TAU/SP/data/STFT'  # '/inputs/TAU/SP/data/stft_norm'
    input_shape = (1, 256, 256)
elif mode == 'Original':
    data_path = '/inputs/TAU/SP/data/original/'
    input_shape = (1, 1000)

super_classes = np.array(["HYP","MI", "NORM", "CD", "STTC"])
#super_classes = np.array(["MI", "NORM", "CD", "STTC"])

MODEL_CKPT_PATH = 'model/'
MODEL_CKPT = 'model/model-{epoch:02d}-{val_loss:.2f}'


for batch_loop in [32]: #[16, 32, 64, 256]:
    for lr_loop in [1e-4]:#, 1e-3, 1e-4, 1e-5]:
        for feature_num in [13]: #[10, 20, 50, 100]:
            for drop_prob_loop in [0.2]:
                print('batch_loop: ', batch_loop)
                print('lr_loop: ', lr_loop)
                print('feature_num: ', feature_num)
                print('drop_prob_loop: ', drop_prob_loop)

                batch_size = batch_loop

                dm = get_data_module(mode, batch_size, data_path)

                # # dm._has_setup_fit = False
                dm.setup()

                label_hist_train = list(np.unique(dm.train.dataset.targets[dm.train.indices], return_counts=True))
                print('label_hist: ', label_hist_train)
                label_hist_train[1] = label_hist_train[1] / sum(label_hist_train[1])
                print(f"weigts_train:{label_hist_train[1]}")

                label_hist_val = list(np.unique(dm.val.dataset.targets[dm.val.indices], return_counts=True))
                print('label_hist: ', label_hist_val)
                label_hist_val[1] = label_hist_val[1] / sum(label_hist_val[1])
                print(f"weigts_val:{label_hist_val[1]}")




                # # Samples required by the custom ImagePredictionLogger callback to log image predictions.
                val_samples = next(iter(dm.val_dataloader()))
                val_imgs, val_labels = val_samples[0], val_samples[1]

                # x = val_imgs[[0]]
                # a = nn.Conv1d(1, 256, 7)
                # b = a(x.unsqueeze(0))
                # print(b.shape)

                # Init our model
                lr = lr_loop
                weight_decay = 0
                drop_prob = drop_prob_loop
                loss_weights_train = torch.cuda.FloatTensor(label_hist_train[1])
                loss_weights_val = torch.cuda.FloatTensor(label_hist_val[1])

                model = get_model(mode, input_shape, len(super_classes), loss_weights_train, loss_weights_val, device, lr, weight_decay, batch_size, drop_prob, feature_num)



                logger = TensorBoardLogger('runs', f'Net:{str(model)}_dataPath:{data_path}_featureNum:{feature_num}_dropProbLoop:{drop_prob_loop}')

                """## Training"""
                print(f'Training Started of {model}!')
                trainer = pl.Trainer(
                    gradient_clip_val=0.5,
                    stochastic_weight_avg=True,
                    logger=logger,    # TB integration
                    log_every_n_steps=1,   # set the logging frequency
                    gpus=1,                # use one GPU
                    max_epochs=400,           # number of epochs
                    # deterministic=True,     # keep it deterministic
                    auto_lr_find=True,
                    callbacks=[
                                ImagePredictionLogger(val_samples, super_classes),
                                #ModelCheckpoint(monitor='val_loss', filename=MODEL_CKPT, save_top_k=1, mode='min')
                                #  EarlyStopping(monitor='val_loss',patience=3,verbose=False,mode='min')
                                ]  # see Callbacks section
                    )

                # Train the model âš¡ðŸš…âš¡
                trainer.fit(model, datamodule=dm)

                # evaluate the model on a test set
                trainer.test(model, datamodule=dm)  # uses last-saved model

print('Training Finished!')
